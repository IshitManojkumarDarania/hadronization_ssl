{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install git+https://github.com/pyg-team/pytorch_geometric.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:12:35.037255Z","iopub.execute_input":"2024-11-13T17:12:35.037807Z","iopub.status.idle":"2024-11-13T17:14:02.326138Z","shell.execute_reply.started":"2024-11-13T17:12:35.037743Z","shell.execute_reply":"2024-11-13T17:14:02.324598Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.4.0+cpu.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_scatter-2.1.2%2Bpt24cpu-cp310-cp310-linux_x86_64.whl (541 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.0/541.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2+pt24cpu\nLooking in links: https://data.pyg.org/whl/torch-2.4.0+cpu.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_sparse-0.6.18%2Bpt24cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.14.1)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-sparse) (1.26.4)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.18+pt24cpu\nLooking in links: https://data.pyg.org/whl/torch-2.4.0+cpu.html\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_cluster-1.6.3%2Bpt24cpu-cp310-cp310-linux_x86_64.whl (787 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.5/787.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-cluster) (1.14.1)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-cluster) (1.26.4)\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt24cpu\nCollecting git+https://github.com/pyg-team/pytorch_geometric.git\n  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-tcx_rle5\n  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-tcx_rle5\n  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 0fa52cb408b6ad3627e200ba6161ca5c8571740b\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.7.0) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.7.0) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric==2.7.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.7.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.7.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.7.0) (2024.8.30)\nBuilding wheels for collected packages: torch-geometric\n  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1136928 sha256=3324ad05f75d4ee1c45576c0ed806ff4e13094a8b54943f6f4c85826e759a71d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-8avy9pn9/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\nSuccessfully built torch-geometric\nInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.7.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install dive-into-graphs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:02.329161Z","iopub.execute_input":"2024-11-13T17:14:02.329976Z","iopub.status.idle":"2024-11-13T17:14:35.488373Z","shell.execute_reply.started":"2024-11-13T17:14:02.329929Z","shell.execute_reply":"2024-11-13T17:14:35.486260Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting dive-into-graphs\n  Downloading dive_into_graphs-1.1.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (1.14.1)\nCollecting cilog (from dive-into-graphs)\n  Downloading cilog-1.3.0-py3-none-any.whl.metadata (9.9 kB)\nCollecting typed-argument-parser==1.7.2 (from dive-into-graphs)\n  Downloading typed-argument-parser-1.7.2.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting captum==0.2.0 (from dive-into-graphs)\n  Downloading captum-0.2.0-py3-none-any.whl.metadata (22 kB)\nCollecting munch (from dive-into-graphs)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nCollecting gdown (from dive-into-graphs)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: shap in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (0.44.1)\nRequirement already satisfied: IPython in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (8.21.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (4.66.4)\nCollecting rdkit-pypi (from dive-into-graphs)\n  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (2.2.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from dive-into-graphs) (1.12)\nCollecting pyscf>=1.7.6 (from dive-into-graphs)\n  Downloading pyscf-2.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nCollecting hydra-core (from dive-into-graphs)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum==0.2.0->dive-into-graphs) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum==0.2.0->dive-into-graphs) (1.26.4)\nRequirement already satisfied: torch>=1.2 in /opt/conda/lib/python3.10/site-packages (from captum==0.2.0->dive-into-graphs) (2.4.0+cpu)\nRequirement already satisfied: typing_extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from typed-argument-parser==1.7.2->dive-into-graphs) (4.12.2)\nRequirement already satisfied: typing-inspect>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from typed-argument-parser==1.7.2->dive-into-graphs) (0.9.0)\nRequirement already satisfied: h5py>=2.7 in /opt/conda/lib/python3.10/site-packages (from pyscf>=1.7.6->dive-into-graphs) (3.11.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from pyscf>=1.7.6->dive-into-graphs) (70.0.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from cilog->dive-into-graphs) (0.9.0)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from cilog->dive-into-graphs) (3.1.5)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->dive-into-graphs) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->dive-into-graphs) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->dive-into-graphs) (2.32.3)\nCollecting omegaconf<2.4,>=2.2 (from hydra-core->dive-into-graphs)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core->dive-into-graphs)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core->dive-into-graphs) (21.3)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (5.14.3)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from IPython->dive-into-graphs) (4.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->dive-into-graphs) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->dive-into-graphs) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->dive-into-graphs) (2024.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi->dive-into-graphs) (10.3.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap->dive-into-graphs) (1.2.2)\nRequirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap->dive-into-graphs) (0.0.7)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap->dive-into-graphs) (0.60.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap->dive-into-graphs) (3.0.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->dive-into-graphs) (1.3.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->IPython->dive-into-graphs) (0.8.4)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.4,>=2.2->hydra-core->dive-into-graphs) (6.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core->dive-into-graphs) (3.1.2)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->IPython->dive-into-graphs) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->dive-into-graphs) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dive-into-graphs) (1.16.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->captum==0.2.0->dive-into-graphs) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->captum==0.2.0->dive-into-graphs) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->captum==0.2.0->dive-into-graphs) (2024.6.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.7.1->typed-argument-parser==1.7.2->dive-into-graphs) (1.0.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->dive-into-graphs) (2.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.2.0->dive-into-graphs) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.2.0->dive-into-graphs) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.2.0->dive-into-graphs) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.2.0->dive-into-graphs) (1.4.5)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap->dive-into-graphs) (0.43.0)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->cilog->dive-into-graphs) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->dive-into-graphs) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->dive-into-graphs) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->dive-into-graphs) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->dive-into-graphs) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->dive-into-graphs) (1.7.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap->dive-into-graphs) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap->dive-into-graphs) (3.5.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->dive-into-graphs) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->dive-into-graphs) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->dive-into-graphs) (0.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.2->captum==0.2.0->dive-into-graphs) (2.1.5)\nDownloading dive_into_graphs-1.1.0-py3-none-any.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading captum-0.2.0-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyscf-2.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cilog-1.3.0-py3-none-any.whl (14 kB)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nDownloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: typed-argument-parser, antlr4-python3-runtime\n  Building wheel for typed-argument-parser (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for typed-argument-parser: filename=typed_argument_parser-1.7.2-py3-none-any.whl size=22683 sha256=c8a6ac984692b70a523d4581ee4c9a8f08b12387f3a32b74aa031fde3410ffb9\n  Stored in directory: /root/.cache/pip/wheels/7e/3e/65/0bed56e064aec3760b50245803510ee4fa767ea3a475a89256\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=5f4ee8afd6ecae4096082343be732ea23152684c395ffe9801ffd5ebcce446e0\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built typed-argument-parser antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, rdkit-pypi, omegaconf, munch, typed-argument-parser, pyscf, hydra-core, gdown, cilog, captum, dive-into-graphs\nSuccessfully installed antlr4-python3-runtime-4.9.3 captum-0.2.0 cilog-1.3.0 dive-into-graphs-1.1.0 gdown-5.2.0 hydra-core-1.3.2 munch-4.0.0 omegaconf-2.3.0 pyscf-2.7.0 rdkit-pypi-2022.9.5 typed-argument-parser-1.7.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from dig.sslgraph.utils import Encoder\nfrom dig.sslgraph.evaluation import GraphSemisupervised, GraphUnsupervised\nfrom dig.sslgraph.dataset import get_dataset\nfrom dig.sslgraph.method import GraphCL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:35.490374Z","iopub.execute_input":"2024-11-13T17:14:35.490962Z","iopub.status.idle":"2024-11-13T17:14:40.822157Z","shell.execute_reply.started":"2024-11-13T17:14:35.490910Z","shell.execute_reply":"2024-11-13T17:14:40.820780Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os.path as osp\nfrom itertools import repeat\nimport os, shutil, torch\nimport numpy as np\n\nfrom torch_geometric.data import InMemoryDataset, download_url, extract_zip\nfrom torch_geometric.io import read_tu_data\n\n\nclass TUDatasetExt(InMemoryDataset):\n    r\"\"\"An extended TUDataset from `Pytorch Geometric \n    <https://pytorch-geometric.readthedocs.io/en/latest/index.html>`_, including\n    a variety of graph kernel benchmark datasets, *e.g.* \"IMDB-BINARY\", \n    \"REDDIT-BINARY\" or \"PROTEINS\".\n\n    .. note::\n        Some datasets may not come with any node labels.\n        You can then either make use of the argument :obj:`use_node_attr`\n        to load additional continuous node attributes (if present) or provide\n        synthetic node features using transforms such as\n        like :class:`torch_geometric.transforms.Constant` or\n        :class:`torch_geometric.transforms.OneHotDegree`.\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The `name\n            <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n            dataset.\n        task (string): The evaluation task. Either 'semisupervised' or\n            'unsupervised'.\n        transform (callable, optional): A function/transform that takes in an\n            :obj:`torch_geometric.data.Data` object and returns a transformed\n            version. The data object will be transformed before every access.\n            (default: :obj:`None`)\n        pre_transform (callable, optional): A function/transform that takes in\n            an :obj:`torch_geometric.data.Data` object and returns a\n            transformed version. The data object will be transformed before\n            being saved to disk. (default: :obj:`None`)\n        pre_filter (callable, optional): A function that takes in an\n            :obj:`torch_geometric.data.Data` object and returns a boolean\n            value, indicating whether the data object should be included in the\n            final dataset. (default: :obj:`None`)\n        use_node_attr (bool, optional): If :obj:`True`, the dataset will\n            contain additional continuous node attributes (if present).\n            (default: :obj:`False`)\n        use_edge_attr (bool, optional): If :obj:`True`, the dataset will\n            contain additional continuous edge attributes (if present).\n            (default: :obj:`False`)\n        cleaned (bool, optional): If :obj:`True`, the dataset will\n            contain only non-isomorphic graphs. (default: :obj:`False`)\n        processed_filename (string, optional): The name of the processed data file.\n            (default: obj: `data.pt`)\n    \"\"\"\n    url = 'https://www.chrsmrrs.com/graphkerneldatasets'\n    cleaned_url = ('https://chrsmrrs.github.io/datasets/docs/datasets/')\n\n    def __init__(self,\n                 root,\n                 name,\n                 task,\n                 transform=None,\n                 pre_transform=None,\n                 pre_filter=None,\n                 use_node_attr=False,\n                 use_edge_attr=False,\n                 cleaned=False,\n                 processed_filename='data.pt'\n                 ):\n        self.processed_filename = processed_filename\n        self.name = name\n        self.cleaned = cleaned\n        self.task = task\n        super(TUDatasetExt, self).__init__(root, transform, pre_transform, pre_filter)\n\n        if self.task == \"semisupervised\":\n            self.data, self.slices = torch.load(self.processed_paths[0])\n            if self.data.x is not None and not use_node_attr:\n                num_node_attributes = self.num_node_attributes\n                self.data.x = self.data.x[:, num_node_attributes:]\n            if self.data.edge_attr is not None and not use_edge_attr:\n                num_edge_attributes = self.num_edge_attributes\n                self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n\n        elif self.task == \"unsupervised\":\n            self.data, self.slices = torch.load(self.processed_paths[0])\n            if self.data.x is not None and not use_node_attr:\n                num_node_attributes = self.num_node_attributes\n                self.data.x = self.data.x[:, num_node_attributes:]\n            if self.data.edge_attr is not None and not use_edge_attr:\n                num_edge_attributes = self.num_edge_attributes\n                self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n            if self.data.x is None:\n                edge_index = self.data.edge_index[0, :].numpy()\n                _, num_edge = self.data.edge_index.size()\n                nlist = [edge_index[n] + 1 for n in range(num_edge - 1) if edge_index[n] > edge_index[n + 1]]\n                nlist.append(edge_index[-1] + 1)\n\n                num_node = np.array(nlist).sum()\n                self.data.x = torch.ones((num_node, 1))\n\n                edge_slice = [0]\n                k = 0\n                for n in nlist:\n                    k = k + n\n                    edge_slice.append(k)\n                self.slices['x'] = torch.tensor(edge_slice)\n        else:\n            ValueError(\"Wrong task name\")\n\n    @property\n    def raw_dir(self):\n        name = 'raw{}'.format('_cleaned' if self.cleaned else '')\n        return osp.join(self.root, self.name, name)\n\n    @property\n    def processed_dir(self):\n        name = 'processed{}'.format('_cleaned' if self.cleaned else '')\n        return osp.join(self.root, self.name, name)\n\n    @property\n    def num_node_labels(self):\n        if self.data.x is None:\n            return 0\n        for i in range(self.data.x.size(1)):\n            x = self.data.x[:, i:]\n            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():\n                return self.data.x.size(1) - i\n        return 0\n\n    @property\n    def num_node_attributes(self):\n        if self.data.x is None:\n            return 0\n        return self.data.x.size(1) - self.num_node_labels\n\n    @property\n    def num_edge_labels(self):\n        if self.data.edge_attr is None:\n            return 0\n        for i in range(self.data.edge_attr.size(1)):\n            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\n                return self.data.edge_attr.size(1) - i\n        return 0\n\n    @property\n    def num_edge_attributes(self):\n        if self.data.edge_attr is None:\n            return 0\n        return self.data.edge_attr.size(1) - self.num_edge_labels\n\n    @property\n    def raw_file_names(self):\n        names = ['A', 'graph_indicator']\n        return ['{}_{}.txt'.format(self.name, name) for name in names]\n\n    @property\n    def processed_file_names(self):\n        return self.processed_filename\n\n    def download(self):\n        url = self.cleaned_url if self.cleaned else self.url\n        folder = osp.join(self.root, self.name)\n        path = download_url('{}/{}.zip'.format(url, self.name), folder)\n        extract_zip(path, folder)\n        os.unlink(path)\n        shutil.rmtree(self.raw_dir)\n        os.rename(osp.join(folder, self.name), self.raw_dir)\n\n    def process(self):\n\n        self.data, self.slices, temp = read_tu_data(self.raw_dir, self.name)\n\n        if self.pre_filter is not None:\n            data_list = [self.get(idx) for idx in range(len(self))]\n            data_list = [data for data in data_list if self.pre_filter(data)]\n            self.data, self.slices = self.collate(data_list)\n\n        if self.pre_transform is not None:\n            data_list = [self.get(idx) for idx in range(len(self))]\n            data_list = [self.pre_transform(data) for data in data_list]\n            self.data, self.slices = self.collate(data_list)\n\n        torch.save((self.data, self.slices), self.processed_paths[0])\n\n    def __repr__(self):\n        return '{}({})'.format(self.name, len(self))\n\n    def get_num_feature(self):\n        data = self.data.__class__()\n\n        for key in self.data.keys():\n            item, slices = self.data[key], self.slices[key]\n            if torch.is_tensor(item):\n                s = list(repeat(slice(None), item.dim()))\n                s[self.data.__cat_dim__(key, item)] = slice(slices[0], slices[0 + 1])\n            else:\n                s = slice(slices[idx], slices[idx + 1])\n            data[key] = item[s]\n        _, num_feature = data.x.size()\n\n        return num_feature\n\n    def get(self, idx):\n        data = self.data.__class__()\n\n        for key in self.data.keys():\n            if key == 'num_nodes':\n                continue\n            item, slices = self.data[key], self.slices[key]\n            if torch.is_tensor(item):\n                s = list(repeat(slice(None), item.dim()))\n                s[self.data.__cat_dim__(key, item)] = slice(slices[idx], slices[idx + 1])\n            else:\n                s = slice(slices[idx], slices[idx + 1])\n            data[key] = item[s]\n\n        if self.task == \"unsupervised\":\n            node_num = data.edge_index.max()\n            sl = torch.tensor([[n, n] for n in range(node_num)]).t()\n            data.edge_index = torch.cat((data.edge_index, sl), dim=1)\n\n        return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:40.825867Z","iopub.execute_input":"2024-11-13T17:14:40.826800Z","iopub.status.idle":"2024-11-13T17:14:40.876911Z","shell.execute_reply.started":"2024-11-13T17:14:40.826738Z","shell.execute_reply":"2024-11-13T17:14:40.875565Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.utils import degree, remove_self_loops, add_self_loops\nclass FeatureExpander(MessagePassing):\n    \"\"\"\n    \"\"\"\n    def __init__(self, degree=True, onehot_maxdeg=0, AK=1, centrality=False,\n                remove_edges=\"none\", edge_noises_add=0, edge_noises_delete=0, \n                group_degree=0):\n        super(FeatureExpander, self).__init__()\n\n        self.degree = degree\n        self.onehot_maxdeg = onehot_maxdeg\n        self.AK = AK\n        self.centrality = centrality\n        self.remove_edges = remove_edges\n        self.edge_noises_add = edge_noises_add\n        self.edge_noises_delete = edge_noises_delete\n        self.group_degree = group_degree\n\n        # edge norm is used, and set A diag to it\n        self.edge_norm_diag = 1e-8\n\n    def transform(self, data):\n        if data.x is None:\n            data.x = torch.ones([data.num_nodes, 1], dtype=torch.float)\n        \n        # ignore edge noises\n\n        deg, deg_onehot = self.compute_degree(data.edge_index, data.num_nodes)\n        akx = self.compute_akx(data.num_nodes, data.x, data.edge_index)\n        cent = self.compute_centrality(data)\n        # data.x = torch.cat([data.x, deg, deg_onehot, akx, cent], dim=-1)\n        # data.x = torch.cat([data.x, deg_onehot, akx, cent, deg], dim=-1)\n        data.x = torch.cat([deg, data.x, deg_onehot, akx, cent], dim=-1)\n\n        return data\n    \n    def compute_degree(self, edge_index, num_nodes):\n        row, col = edge_index\n        deg = degree(row, num_nodes)\n        deg = deg.view((-1, 1))\n\n        if self.onehot_maxdeg is not None and self.onehot_maxdeg > 0:\n            max_deg = torch.tensor(self.onehot_maxdeg, dtype=deg.dtype)\n            deg_capped = torch.min(deg, max_deg).type(torch.int64)\n            deg_onehot = nn.functional.one_hot(deg_capped.view(-1), num_classes=self.onehot_maxdeg + 1)\n            deg_onehot = deg_onehot.type(deg.dtype)\n        else:\n            deg_onehot = self.empty_feature(num_nodes)\n\n        if not self.degree:\n            deg = self.empty_feature(num_nodes)\n        \n        return deg, deg_onehot\n\n    \n    def compute_akx(self, num_nodes, x, edge_index, edge_weight=None):\n        if self.AK is None or self.AK <= 0:\n            return self.empty_feature(num_nodes)\n    \n    def compute_centrality(self, data):\n        if not self.centrality:\n            return self.empty_feature(data.num_nodes)\n    \n    def empty_feature(self, num_nodes):\n        return torch.zeros([num_nodes, 0])\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, diag_val=1e-8, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1), ),\n                                     dtype=dtype,\n                                     device=edge_index.device)\n        edge_weight = edge_weight.view(-1)\n        assert edge_weight.size(0) == edge_index.size(1)\n\n        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n        edge_index = add_self_loops(edge_index, num_nodes=num_nodes)\n        # Add edge_weight for loop edges.\n        loop_weight = torch.full((num_nodes, ),\n                                 diag_val,\n                                 dtype=edge_weight.dtype,\n                                 device=edge_weight.device)\n        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n\nclass CatDegOnehot(object):\n    r\"\"\"Adds the node degree as one hot encodings to the node features.\n\n    Args:\n        max_degree (int): Maximum degree.\n        in_degree (bool, optional): If set to :obj:`True`, will compute the\n            in-degree of nodes instead of the out-degree.\n            (default: :obj:`False`)\n        cat (bool, optional): Concat node degrees to node features instead\n            of replacing them. (default: :obj:`True`)\n    \"\"\"\n\n    def __init__(self, max_degree, in_degree=False, cat=True):\n        self.max_degree = max_degree\n        self.in_degree = in_degree\n        self.cat = cat\n\n    def __call__(self, data):\n        idx, x = data.edge_index[1 if self.in_degree else 0], data.x\n        deg = degree(idx, data.num_nodes, dtype=torch.long)\n        deg = F.one_hot(deg, num_classes=self.max_degree + 1).to(torch.float)\n\n        if x is not None and self.cat:\n            x = x.view(-1, 1) if x.dim() == 1 else x\n            data.x = torch.cat([x, deg.to(x.dtype)], dim=-1)\n        else:\n            data.x = deg\n            \n        return data\n\n    def __repr__(self):\n        return '{}({})'.format(self.__class__.__name__, self.max_degree)\n    \n    \ndef get_max_deg(dataset):\n    max_deg = 0\n    for data in dataset:\n        row, col = data.edge_index\n        num_nodes = data.num_nodes\n        deg = degree(row, num_nodes)\n        deg = max(deg).item()\n        if deg > max_deg:\n            max_deg = int(deg)\n    return max_deg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:40.878665Z","iopub.execute_input":"2024-11-13T17:14:40.879162Z","iopub.status.idle":"2024-11-13T17:14:40.914138Z","shell.execute_reply.started":"2024-11-13T17:14:40.879103Z","shell.execute_reply":"2024-11-13T17:14:40.912742Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import re\n\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.transforms import NormalizeFeatures\n\n\ndef get_dataset(name, task, feat_str=\"deg\", root=None):\n    r\"\"\"A pre-implemented function to retrieve graph datasets from TUDataset.\n    Depending on evaluation tasks, different node feature augmentation will\n    be applied following `GraphCL <https://arxiv.org/abs/2010.13902>`_.\n\n    Args:\n        name (string): The `name <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the dataset.\n        task (string): The evaluation task. Either 'semisupervised' or\n            'unsupervised'.\n        feat_str (bool, optional): The node feature augmentations to be applied,\n            *e.g.*, degrees and centrality. (default: :obj:`deg`)\n        root (string, optional): Root directory where the dataset should be saved.\n            (default: :obj:`None`)\n        \n    :rtype: :class:`torch_geometric.data.Dataset` (unsupervised), or (:class:`torch_geometric.data.Dataset`, \n        :class:`torch_geometric.data.Dataset`) (semisupervised).\n        \n    Examples\n    --------\n    >>> dataset, dataset_pretrain = get_dataset(\"NCI1\", \"semisupervised\")\n    >>> dataset\n    NCI1(4110)\n    \n    >>> dataset = get_dataset(\"MUTAG\", \"unsupervised\", feat_str=\"\")\n    >>> dataset # degree not augmented as node attributes\n    MUTAG(188)\n    \"\"\"\n\n    root = \".\" if root is None else root\n    if task == \"semisupervised\":\n\n        if name in ['REDDIT-BINARY', 'REDDIT-MULTI-5K', 'REDDIT-MULTI-12K']:\n            feat_str = feat_str.replace('odeg100', 'odeg10')\n        if name in ['DD']:\n            feat_str = feat_str.replace('odeg100', 'odeg10')\n            feat_str = feat_str.replace('ak3', 'ak1')\n\n        degree = feat_str.find(\"deg\") >= 0\n        onehot_maxdeg = re.findall(\"odeg(\\d+)\", feat_str)\n        onehot_maxdeg = int(onehot_maxdeg[0]) if onehot_maxdeg else None\n\n        pre_transform = FeatureExpander(degree=degree, \n                                        onehot_maxdeg=onehot_maxdeg, AK=0).transform\n\n        dataset = TUDatasetExt(root+\"/semi_dataset/dataset\", name, task, \n                               pre_transform=pre_transform, use_node_attr=True,\n                               processed_filename=\"data_%s.pt\" % feat_str)\n\n        dataset_pretrain = TUDatasetExt(root+\"/semi_dataset/pretrain_dataset/\", name, task, \n                                        pre_transform=pre_transform, use_node_attr=True,\n                                        processed_filename=\"data_%s.pt\" % feat_str)\n\n        dataset.data.edge_attr = None\n        dataset_pretrain.data.edge_attr = None\n\n        return dataset, dataset_pretrain\n\n    elif task == \"unsupervised\":\n        dataset = TUDatasetExt(root+\"/unsuper_dataset/\", name=name, task=task)\n        if feat_str.find(\"deg\") >= 0:\n            max_degree = get_max_deg(dataset)\n            dataset = TUDatasetExt(root+\"./unsuper_dataset/\", name=name, task=task,\n                                   transform=CatDegOnehot(max_degree), use_node_attr=True)\n        return dataset\n\n    else:\n        ValueError(\"Wrong task name\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:40.915967Z","iopub.execute_input":"2024-11-13T17:14:40.916456Z","iopub.status.idle":"2024-11-13T17:14:40.934569Z","shell.execute_reply.started":"2024-11-13T17:14:40.916393Z","shell.execute_reply":"2024-11-13T17:14:40.933012Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset, dataset_pretrain = get_dataset('NCI1', task='semisupervised')\nfeat_dim = dataset[0].x.shape[1]\nembed_dim = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:40.936278Z","iopub.execute_input":"2024-11-13T17:14:40.936815Z","iopub.status.idle":"2024-11-13T17:14:47.612519Z","shell.execute_reply.started":"2024-11-13T17:14:40.936747Z","shell.execute_reply":"2024-11-13T17:14:47.611249Z"}},"outputs":[{"name":"stderr","text":"Downloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\nExtracting semi_dataset/dataset/NCI1/NCI1.zip\nProcessing...\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nDone!\n/tmp/ipykernel_29/3960502999.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.data, self.slices = torch.load(self.processed_paths[0])\nDownloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\nExtracting semi_dataset/pretrain_dataset/NCI1/NCI1.zip\nProcessing...\nDone!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"encoder = Encoder(feat_dim, embed_dim, n_layers=3, gnn='resgcn')\ngraphcl = GraphCL(embed_dim, aug_1='random2', aug_2='random2')\nevaluator = GraphSemisupervised(dataset, dataset_pretrain, label_rate=0.1)\nevaluator.evaluate(learning_model=graphcl, encoder=encoder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:14:47.614194Z","iopub.execute_input":"2024-11-13T17:14:47.614607Z","iopub.status.idle":"2024-11-13T17:38:41.467269Z","shell.execute_reply.started":"2024-11-13T17:14:47.614564Z","shell.execute_reply":"2024-11-13T17:38:41.465954Z"}},"outputs":[{"name":"stderr","text":"Pretraining: epoch 100: 100%|██████████| 100/100 [15:28<00:00,  9.29s/it, loss=1.868740]\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nFold 1, finetuning:   0%|          | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nFold 1, finetuning: 100%|██████████| 100/100 [00:51<00:00,  1.95it/s, acc=0.7956, val_loss=0.7210]\nFold 2, finetuning: 100%|██████████| 100/100 [00:53<00:00,  1.89it/s, acc=0.7178, val_loss=1.7133]\nFold 3, finetuning: 100%|██████████| 100/100 [00:51<00:00,  1.93it/s, acc=0.7445, val_loss=1.3189]\nFold 4, finetuning: 100%|██████████| 100/100 [00:51<00:00,  1.95it/s, acc=0.7397, val_loss=1.4330]\nFold 5, finetuning: 100%|██████████| 100/100 [00:51<00:00,  1.96it/s, acc=0.7372, val_loss=1.3224]\nFold 6, finetuning: 100%|██████████| 100/100 [00:49<00:00,  2.02it/s, acc=0.7080, val_loss=1.2336]\nFold 7, finetuning: 100%|██████████| 100/100 [00:49<00:00,  2.03it/s, acc=0.7251, val_loss=1.6672]\nFold 8, finetuning: 100%|██████████| 100/100 [00:49<00:00,  2.04it/s, acc=0.7032, val_loss=1.5122]\nFold 9, finetuning: 100%|██████████| 100/100 [00:49<00:00,  2.04it/s, acc=0.7445, val_loss=1.2468]\nFold 10, finetuning: 100%|██████████| 100/100 [00:49<00:00,  2.02it/s, acc=0.7445, val_loss=1.1939]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(0.7428223490715027, 0.02922072634100914)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"encoder = Encoder(feat_dim, embed_dim, n_layers=3, gnn='resgcn')\ngraphcl = GraphCL(embed_dim, aug_1='subgraph', aug_2='subgraph')\nevaluator = GraphSemisupervised(dataset, dataset_pretrain, label_rate=0.01)\nevaluator.evaluate(learning_model=graphcl, encoder=encoder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T17:38:41.468918Z","iopub.execute_input":"2024-11-13T17:38:41.469303Z","iopub.status.idle":"2024-11-13T18:03:26.317749Z","shell.execute_reply.started":"2024-11-13T17:38:41.469263Z","shell.execute_reply":"2024-11-13T18:03:26.316381Z"}},"outputs":[{"name":"stderr","text":"Pretraining: epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nPretraining: epoch 100: 100%|██████████| 100/100 [19:22<00:00, 11.62s/it, loss=2.813298]\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nFold 1, finetuning:   0%|          | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\nFold 1, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.07it/s, acc=0.6545, val_loss=8.3511]\nFold 2, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.06it/s, acc=0.6472, val_loss=12.4460]\nFold 3, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.11it/s, acc=0.5377, val_loss=5.2754]\nFold 4, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.12it/s, acc=0.5839, val_loss=3.8150]\nFold 5, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.10it/s, acc=0.6180, val_loss=7.2251]\nFold 6, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.11it/s, acc=0.6521, val_loss=3.9511]\nFold 7, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.12it/s, acc=0.6472, val_loss=4.7816]\nFold 8, finetuning: 100%|██████████| 100/100 [00:31<00:00,  3.15it/s, acc=0.6131, val_loss=4.1187]\nFold 9, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.12it/s, acc=0.6813, val_loss=2.1483]\nFold 10, finetuning: 100%|██████████| 100/100 [00:32<00:00,  3.08it/s, acc=0.6131, val_loss=2.7414]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(0.6343066096305847, 0.04836418852210045)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}